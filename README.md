# TFM_bot

# Instrucciones de uso del proyecto RAG + Ollama

Este documento explica c贸mo ejecutar y probar el proyecto de recuperaci贸n aumentada por generaci贸n (RAG) usando modelos locales con Ollama.

## 1. Iniciar el entorno Docker

Abre una terminal (cmd o PowerShell) y sigue estos pasos:

```bash
cd ruta/del/proyecto
docker compose up
```

Esto levantar谩 el contenedor de Docker con el entorno necesario.

## 2. Acceder al contenedor Ollama

En otra terminal (cmd o PowerShell), ejecuta el siguiente comando para entrar dentro del contenedor:

```bash
docker exec -it ollama-container bash
```

## 3. Ver y gestionar los modelos en Ollama

Dentro del contenedor, puedes listar los modelos descargados con:

```bash
ollama list
```

Si no tienes ning煤n modelo, puedes descargarlos con:

```bash
ollama pull deepseek-r1:1.5b
ollama pull llama3.2:1b
```

Para eliminar un modelo:

```bash
ollama rm nombre-del-modelo
```

>  Una vez llegues a este punto, el entorno en Docker ya est谩 configurado.

## 4. Preprocesamiento de PDFs

En el corpus ya est谩n preprocesados los PDFs. El c贸digo relacionado con esta parte est谩 en:

- `extracci贸n_docling.py`: convierte el contenido de los PDFs a texto plano.
- `table_to_text.py`: transforma tablas en oraciones con sentido.

## 5. Persistencia de embeddings en la base de datos

Ejecuta el siguiente script para guardar los embeddings en la base de datos Chroma:

```bash
python persist_chroma.py
```

## 6. Probar el sistema RAG

Para probar el sistema por l铆nea de comandos:

```bash
python rag_query.py
```

## 7. Si se quiere ejecutar la aplicaci贸n web con Streamlit

Si quieres probar la interfaz visual, ejecuta:

```bash
streamlit run app.py
```

---

锔 **Nota:**  
Este c贸digo est谩 sujeto a cambios y mejoras. Es una primera versi贸n provisional mientras se avanza en la redacci贸n de la memoria del proyecto.
